
<!DOCTYPE html>
<html lang='en-US'>
<head>
  <meta charset='utf-8'>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Mechanistic Interpretability - Some concepts</title>
  <meta name="description" content="Here are some quick notes on concepts in Mechanistic Interpretability. The subject is vast and very recent and 
try to interpret features for neural networks, specifically transformers and LLM's.">
  <link rel="icon" href="/favicon.png" type="image/png">
  <link rel="icon" href="/favicon.svg" type="image/svg+xml">
  <link rel="canonical" href="https://mbottoni.github.io/2025/04/12/mech-inter.html">
  <link rel="alternate" type="application/rss+xml" title="mbottoni" href="https://mbottoni.github.io/feed.xml">
  <style>
  @font-face {
    font-family: 'Open Sans'; src: url('/css/OpenSans-300-Normal.woff2') format('woff2');
    font-weight: 300; font-style: normal;
  }
  @font-face {
    font-family: 'JetBrains Mono'; src: url('/css/JetBrainsMono-400-Normal.woff2') format('woff2');
    font-weight: 400; font-style: normal;
  }
  @font-face {
    font-family: 'JetBrains Mono'; src: url('/css/JetBrainsMono-700-Normal.woff2') format('woff2');
    font-weight: 700; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-400-Normal.woff2') format('woff2');
    font-weight: 400; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-400-Italic.woff2') format('woff2');
    font-weight: 400; font-style: italic;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-700-Normal.woff2') format('woff2');
    font-weight: 700; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-700-Italic.woff2') format('woff2');
    font-weight: 700; font-style: italic;
  }

  * { box-sizing: border-box; margin: 0; padding: 0; margin-block-start: 0; margin-block-end: 0; }

  body {
    max-width: 80ch;
    padding: 2ch;
    margin-left: auto;
    margin-right: auto;
  }

  header { margin-bottom: 2rem; }
  header > nav { display: flex; column-gap: 2ch; align-items: baseline; flex-wrap: wrap; }
  header a { font-style: normal; color: rgba(0, 0, 0, .8); text-decoration: none; }
  header a:hover { color: rgba(0, 0, 0, .8); text-decoration: underline; }
  header .title { font-size: 1.25em; flex-grow: 2; }

  footer { margin-top: 2rem; }
  footer > p { display: flex; column-gap: 2ch; justify-content: center; flex-wrap: wrap; }
  footer a { color: rgba(0, 0, 0, .8); text-decoration: none; white-space: nowrap; }
  footer i { vertical-align: middle; color: rgba(0, 0, 0, .8) }

  </style>

  <link rel="stylesheet" href="/css/main.css">
  
</head>

<body>
  <header>
    <nav>
      <a class="title" href="/">mbottoni</a>
      <a href="/about.html">About</a>
      <a href="/resume.html">Resume</a>
      <a href="/links.html">Links</a>
    </nav>
  </header>

  <main>
  <article >

    <h1>
    <a href="#Mechanistic-Interpretability-Some-concepts"><span>Mechanistic Interpretability - Some concepts</span> <time datetime="2025-04-12">Apr 12, 2025</time></a>
    </h1>
<p><span>Here are some quick notes on concepts in Mechanistic Interpretability. The subject is vast and very recent and </span>
<span>try to interpret features for neural networks, specifically transformers and LLM</span>&rsquo;<span>s.</span></p>
<p><span>This was based on my studies and mainly on this blog post </span><a href="https://www.neelnanda.io/mechanistic-interpretability/glossary"><span>Link to blog</span></a><span>. On the future when I advanced</span>
<span>my studies on this subject I will update this post.</span></p>
<ul>
<li>
<p><span>Interpretability: The broader AI subfield focused on understanding why AI systems behave as they do and translating this into human-understandable explanations.</span></p>
</li>
<li>
<p><span>Feature: A property of the model</span>&rsquo;<span>s input or a part of it. Features are fundamental; the model</span>&rsquo;<span>s internal activations represent them, and computations (weights, non-linearities) transform earlier features into later ones.</span></p>
</li>
<li>
<p><span>Circuit: A specific part of the model that performs an understandable computation, transforming interpretable input features into interpretable output features.</span></p>
</li>
<li>
<p><span>Intervening on or editing an activation: The process of running the model up to a certain point, modifying a specific activation value, and then letting the model continue its computation with the changed value.</span></p>
</li>
<li>
<p><span>Pruning a neuron: A specific intervention where a neuron</span>&rsquo;<span>s activation is set to zero, preventing subsequent layers from using its output.</span></p>
</li>
<li>
<p><span>Equivariance / Neuron families: Groups of neurons or features that are distinct but operate analogously. Understanding one member of the family helps understand the others.</span></p>
</li>
<li>
<p><span>Neuron splitting: When a single feature represented in a smaller model gets broken down into multiple distinct features in a larger model.</span></p>
</li>
<li>
<p><span>Universality: The hypothesis that the same functional circuits will emerge and be discoverable across different models trained independently.</span></p>
</li>
<li>
<p><span>Motif: An abstract computational pattern or structure that recurs across different circuits or features in various models or contexts.</span></p>
</li>
<li>
<p><span>Localised/sparse model behaviour: When a model</span>&rsquo;<span>s specific behavior is determined by only a small number of its components.</span></p>
</li>
<li>
<p><span>Microscope AI: The idea that by reverse engineering a highly capable (potentially superhuman) AI, we could learn the novel knowledge and understanding of the world that the AI has acquired.</span></p>
</li>
<li>
<p><span>The curse of dimensionality: A concept highlighting the counter-intuitive properties and complexities that arise when dealing with high-dimensional spaces, like the activation spaces within neural networks.</span></p>
</li>
<li>
<p><span>Features as directions: A key hypothesis in MI suggesting that features within a model are represented as specific directions within the high-dimensional activation vector space.</span></p>
</li>
<li>
<p><span>Few-shot learning: A capability where a generative model learns to perform a new task based on just a few examples provided within its input prompt.</span></p>
</li>
<li>
<p><span>In-Context Learning: The ability of a model to use information from much earlier in its input sequence (context) to make predictions about the next token.</span></p>
</li>
<li>
<p><span>Indirect Object Identification (IOI): A specific linguistic task used to test language models, requiring the model to identify the indirect object in sentences like </span>&ldquo;<span>When John and Mary went to the store, John gave the bag to [Mary]</span>&rdquo;<span>.</span></p>
</li>
<li>
<p><span>IOI Circuit: The specific network of components identified within the GPT-2 Small model that is responsible for correctly performing the IOI task.</span></p>
</li>
</ul>
</article>
  </main>

  <footer class="site-footer">
    <p>
      <a href="https://github.com/mbottoni/mbottoni.github.io/edit/master/content/posts/2025-04-12-mech-inter.dj">
        <svg class="icon"><use href="/assets/icons.svg#edit"/></svg>
        Fix typo
      </a>
      <a href="/feed.xml">
        <svg class="icon"><use href="/assets/icons.svg#rss"/></svg>
        Subscribe
      </a>
      <a href="mailto:maruanbakriottoni@gmail.com">
        <svg class="icon"><use href="/assets/icons.svg#email"/></svg>
        Get in touch
      </a>
      <a href="https://github.com/mbottoni">
        <svg class="icon"><use href="/assets/icons.svg#github"/></svg>
        mbottoni
      </a>
    </p>
  </footer>
</body>

</html>
