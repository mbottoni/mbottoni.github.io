
<!DOCTYPE html>
<html lang='en-US'>
<head>
  <meta charset='utf-8'>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Group Relative Policy Optimization (GRPO)</title>
  <meta name="description" content="PPO is a reinforcement learning algorithm originally designed to update policies in a stable and reliable way.
In the context of LLM fine-tuning, the model (the “policy”) is trained using feedback from a reward model that represents human preferences.
Value Function (Critic): Estimates the “goodness” of a state, used with Generalized Advantage Estimation (GAE) to balance bias and variance.
Basically it works as follows:">
  <link rel="icon" href="/favicon.png" type="image/png">
  <link rel="icon" href="/favicon.svg" type="image/svg+xml">
  <link rel="canonical" href="https://mbottoni.github.io/2025/02/06/grpo.html">
  <link rel="alternate" type="application/rss+xml" title="mbottoni" href="https://mbottoni.github.io/feed.xml">
  <style>
  @font-face {
    font-family: 'Open Sans'; src: url('/css/OpenSans-300-Normal.woff2') format('woff2');
    font-weight: 300; font-style: normal;
  }
  @font-face {
    font-family: 'JetBrains Mono'; src: url('/css/JetBrainsMono-400-Normal.woff2') format('woff2');
    font-weight: 400; font-style: normal;
  }
  @font-face {
    font-family: 'JetBrains Mono'; src: url('/css/JetBrainsMono-700-Normal.woff2') format('woff2');
    font-weight: 700; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-400-Normal.woff2') format('woff2');
    font-weight: 400; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-400-Italic.woff2') format('woff2');
    font-weight: 400; font-style: italic;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-700-Normal.woff2') format('woff2');
    font-weight: 700; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-700-Italic.woff2') format('woff2');
    font-weight: 700; font-style: italic;
  }

  * { box-sizing: border-box; margin: 0; padding: 0; margin-block-start: 0; margin-block-end: 0; }

  body {
    max-width: 80ch;
    padding: 2ch;
    margin-left: auto;
    margin-right: auto;
  }

  header { margin-bottom: 2rem; }
  header > nav { display: flex; column-gap: 2ch; align-items: baseline; flex-wrap: wrap; }
  header a { font-style: normal; color: rgba(0, 0, 0, .8); text-decoration: none; }
  header a:hover { color: rgba(0, 0, 0, .8); text-decoration: underline; }
  header .title { font-size: 1.25em; flex-grow: 2; }

  footer { margin-top: 2rem; }
  footer > p { display: flex; column-gap: 2ch; justify-content: center; flex-wrap: wrap; }
  footer a { color: rgba(0, 0, 0, .8); text-decoration: none; white-space: nowrap; }
  footer i { vertical-align: middle; color: rgba(0, 0, 0, .8) }

  </style>

  <link rel="stylesheet" href="/css/main.css">
  
</head>

<body>
  <header>
    <nav>
      <a class="title" href="/">mbottoni</a>
      <a href="/about.html">About</a>
      <a href="/resume.html">Resume</a>
      <a href="/links.html">Links</a>
    </nav>
  </header>

  <main>
  <article >

    <h1>
    <a href="#GRPO-on-DeepSeek-r1"><span>GRPO on DeepSeek-r1</span> <time datetime="2025-02-06">Feb 6, 2025</time></a>
    </h1>

<figure>

<img alt="" src="/assets/grpo.webp">
</figure>

    <h1>
    <a href="#Proximal-Policy-Optimization-PPO"><span>Proximal Policy Optimization (PPO)</span> <time datetime="2025-02-06">Feb 6, 2025</time></a>
    </h1>
<p><span>PPO is a reinforcement learning algorithm originally designed to update policies in a stable and reliable way.</span>
<span>In the context of LLM fine-tuning, the model (the “policy”) is trained using feedback from a reward model that represents human preferences.</span>
<span>Value Function (Critic): Estimates the “goodness” of a state, used with Generalized Advantage Estimation (GAE) to balance bias and variance.</span>
<span>Basically it works as follows:</span></p>
<ol>
<li>
<span>Generate Rollouts: The LLM produces a set of text responses (rollouts) for a given prompt.</span>
</li>
<li>
<span>Score with the Reward Model: Each response is scored.</span>
</li>
<li>
<span>Compute Advantages: Using GAE, the algorithm computes how much better (or worse) each action (e.g., token choice) was compared to a baseline.</span>
</li>
<li>
<span>Update the Policy: A PPO objective is used with a clipped surrogate loss to ensure updates are not too drastic. Additional terms (like a KL penalty and entropy bonus) help maintain stability and encourage exploration.</span>
</li>
</ol>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">ppo_loss_with_gae_entropy</span>(<span class="hl-params">old_policy_logprobs, new_policy_logprobs, advantages,</span></span>
<span class="line"><span class="hl-params">                              kl_penalty_coef, clip_epsilon, entropy_bonus_coef</span>):</span>
<span class="line">    <span class="hl-comment"># Compute probability ratio between new and old policy actions</span></span>
<span class="line">    ratio = np.exp(new_policy_logprobs - old_policy_logprobs)</span>
<span class="line">    </span>
<span class="line">    <span class="hl-comment"># Clipped surrogate objective prevents large policy updates</span></span>
<span class="line">    surrogate_objective = np.minimum(ratio * advantages,</span>
<span class="line">                                     np.clip(ratio, <span class="hl-number">1</span> - clip_epsilon, <span class="hl-number">1</span> + clip_epsilon) * advantages)</span>
<span class="line">    policy_loss = -np.mean(surrogate_objective)</span>
<span class="line">    </span>
<span class="line">    <span class="hl-comment"># KL divergence penalty to keep new policy close to the old one</span></span>
<span class="line">    kl_divergence = np.mean(new_policy_logprobs - old_policy_logprobs)</span>
<span class="line">    kl_penalty = kl_penalty_coef * kl_divergence</span>
<span class="line">    </span>
<span class="line">    <span class="hl-comment"># Entropy bonus encourages exploration (i.e., less certainty)</span></span>
<span class="line">    entropy = -np.mean(new_policy_logprobs)</span>
<span class="line">    entropy_bonus = entropy_bonus_coef * entropy</span>
<span class="line">    </span>
<span class="line">    total_loss = policy_loss + kl_penalty - entropy_bonus</span>
<span class="line">    <span class="hl-keyword">return</span> total_loss</span></code></pre>

</figure>

    <h1>
    <a href="#Direct-Preference-Optimization-DPO"><span>Direct Preference Optimization (DPO)</span> <time datetime="2025-02-06">Feb 6, 2025</time></a>
    </h1>
<p><span>DPO simplifies the training loop by skipping the reinforcement learning (RL) loop entirely.</span>
<span>Rather than estimating rewards and advantages, DPO directly uses human preference data (pairs of preferred vs. dispreferred responses) to adjust the model.</span>
<span>It compares the raw model outputs (logits) between a current model and a reference model (often an earlier, supervised-fine-tuned version) using a loss function similar to binary cross-entropy.</span>
<span>So basically it works as follows:</span></p>
<ol>
<li>
<span>Collect Preference Data: Gather pairs of responses where human judges indicate which response they prefer.</span>
</li>
<li>
<span>Evaluate with Two Models: Compute the logits for both the preferred and dispreferred responses using the current model and a reference model.</span>
</li>
<li>
<span>Compute Log Ratios: Determine the relative “strength” (log probabilities) of preferred versus dispreferred responses.</span>
</li>
<li>
<span>Optimize Directly: Use a loss function that, in effect, makes the current model more likely to generate preferred responses and less likely to generate dispreferred ones—all while staying close to the reference model.</span>
</li>
</ol>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">dpo_loss</span>(<span class="hl-params">policy_logits_preferred, policy_logits_dispreferred,</span></span>
<span class="line"><span class="hl-params">             ref_logits_preferred, ref_logits_dispreferred, beta_kl</span>):</span>
<span class="line">    <span class="hl-comment"># Convert logits to log probabilities (details abstracted)</span></span>
<span class="line">    policy_logprob_preferred = F.log_softmax(policy_logits_preferred, dim=-<span class="hl-number">1</span>).gather(...)</span>
<span class="line">    policy_logprob_dispreferred = F.log_softmax(policy_logits_dispreferred, dim=-<span class="hl-number">1</span>).gather(...)</span>
<span class="line">    ref_policy_logprob_preferred = F.log_softmax(ref_logits_preferred, dim=-<span class="hl-number">1</span>).gather(...)</span>
<span class="line">    ref_policy_logprob_dispreferred = F.log_softmax(ref_logits_dispreferred, dim=-<span class="hl-number">1</span>).gather(...)</span>
<span class="line">    </span>
<span class="line">    <span class="hl-comment"># Calculate the log ratio comparing current model differences to reference differences</span></span>
<span class="line">    log_ratio = (policy_logprob_preferred - policy_logprob_dispreferred -</span>
<span class="line">                 (ref_policy_logprob_preferred - ref_policy_logprob_dispreferred))</span>
<span class="line">    </span>
<span class="line">    <span class="hl-comment"># Convert log ratio into a probability using a logistic function (Bradley-Terry model)</span></span>
<span class="line">    preference_prob = <span class="hl-number">1</span> / (<span class="hl-number">1</span> + np.exp(-beta_kl * log_ratio))</span>
<span class="line">    </span>
<span class="line">    <span class="hl-comment"># Compute loss as binary cross-entropy (minimizing negative log probability of preferred response)</span></span>
<span class="line">    loss = -np.log(preference_prob + <span class="hl-number">1e-8</span>)</span>
<span class="line">    <span class="hl-keyword">return</span> loss</span></code></pre>

</figure>

    <h1>
    <a href="#Group-Relative-Policy-Optimization-GRPO"><span>Group Relative Policy Optimization (GRPO)</span> <time datetime="2025-02-06">Feb 6, 2025</time></a>
    </h1>
<p><span>GRPO is a twist on PPO that is designed to be leaner and faster—especially for complex reasoning tasks.</span>
<span>Instead of relying on a separate value function (critic) to estimate advantages, GRPO uses a group-based approach:</span>
<span>For a given prompt, a group of responses is generated. These responses are scored using the reward model.</span>
<span>So basically it works as follows:</span>
<span>1. Generate a Group of Responses: For each prompt, sample several responses.</span>
<span>2. Score the Responses: Use a reward model to assign a reward to each response.</span>
<span>3. Compute Group Relative Advantages: Calculate advantages by comparing each response’s reward to the group mean (and optionally normalizing by the standard deviation).</span>
<span>4. Update the Policy: Use a PPO-like objective function that takes these group relative advantages into account.</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">grae_advantages</span>(<span class="hl-params">rewards</span>):</span>
<span class="line">    <span class="hl-string">&quot;&quot;&quot;</span></span>
<span class="line"><span class="hl-string">    Compute Group Relative Advantages by normalizing rewards within a group.</span></span>
<span class="line"><span class="hl-string">    &quot;&quot;&quot;</span></span>
<span class="line">    mean_reward = np.mean(rewards)</span>
<span class="line">    std_reward = np.std(rewards)</span>
<span class="line">    normalized_rewards = (rewards - mean_reward) / (std_reward + <span class="hl-number">1e-8</span>)</span>
<span class="line">    advantages = normalized_rewards  <span class="hl-comment"># Here, advantage = normalized reward</span></span>
<span class="line">    <span class="hl-keyword">return</span> advantages</span></code></pre>

</figure>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">grpo_loss</span>(<span class="hl-params">old_policy_logprobs_group, new_policy_logprobs_group,</span></span>
<span class="line"><span class="hl-params">              group_advantages, kl_penalty_coef, clip_epsilon</span>):</span>
<span class="line">    <span class="hl-string">&quot;&quot;&quot;</span></span>
<span class="line"><span class="hl-string">    Compute the GRPO loss over a group of responses.</span></span>
<span class="line"><span class="hl-string">    &quot;&quot;&quot;</span></span>
<span class="line">    group_loss = <span class="hl-number">0</span></span>
<span class="line">    <span class="hl-comment"># Loop over each response in the group</span></span>
<span class="line">    <span class="hl-keyword">for</span> i <span class="hl-keyword">in</span> <span class="hl-built_in">range</span>(<span class="hl-built_in">len</span>(group_advantages)):</span>
<span class="line">        advantage = group_advantages[i]</span>
<span class="line">        new_policy_logprob = new_policy_logprobs_group[i]</span>
<span class="line">        old_policy_logprob = old_policy_logprobs_group[i]</span>
<span class="line">        </span>
<span class="line">        <span class="hl-comment"># Compute the probability ratio for the policy update</span></span>
<span class="line">        ratio = np.exp(new_policy_logprob - old_policy_logprob)</span>
<span class="line">        clipped_ratio = np.clip(ratio, <span class="hl-number">1</span> - clip_epsilon, <span class="hl-number">1</span> + clip_epsilon)</span>
<span class="line">        </span>
<span class="line">        <span class="hl-comment"># Surrogate objective with clipping, similar to PPO</span></span>
<span class="line">        surrogate_objective = np.minimum(ratio * advantage, clipped_ratio * advantage)</span>
<span class="line">        policy_loss = -surrogate_objective</span>
<span class="line">        </span>
<span class="line">        <span class="hl-comment"># KL divergence penalty to restrict too-large updates</span></span>
<span class="line">        kl_divergence = new_policy_logprob - old_policy_logprob</span>
<span class="line">        kl_penalty = kl_penalty_coef * kl_divergence</span>
<span class="line">        </span>
<span class="line">        group_loss += (policy_loss + kl_penalty)</span>
<span class="line">    </span>
<span class="line">    <span class="hl-comment"># Average the loss over the group of responses</span></span>
<span class="line">    <span class="hl-keyword">return</span> group_loss / <span class="hl-built_in">len</span>(group_advantages)</span></code></pre>

</figure>
</article>
  </main>

  <footer class="site-footer">
    <p>
      <a href="https://github.com/mbottoni/mbottoni.github.io/edit/master/content/posts/2025-02-06-grpo.dj">
        <svg class="icon"><use href="/assets/icons.svg#edit"/></svg>
        Fix typo
      </a>
      <a href="/feed.xml">
        <svg class="icon"><use href="/assets/icons.svg#rss"/></svg>
        Subscribe
      </a>
      <a href="mailto:maruanbakriottoni@gmail.com">
        <svg class="icon"><use href="/assets/icons.svg#email"/></svg>
        Get in touch
      </a>
      <a href="https://github.com/mbottoni">
        <svg class="icon"><use href="/assets/icons.svg#github"/></svg>
        mbottoni
      </a>
    </p>
  </footer>
</body>

</html>
