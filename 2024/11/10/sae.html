
<!DOCTYPE html>
<html lang='en-US'>
<head>
  <meta charset='utf-8'>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Sparse Autoencoders</title>
  <meta name="description" content="Sparse autoencoders are neural networks that learn compressed representations of 
data while enforcing sparsity - a constraint that ensures most neurons 
remain inactive for any given input. This approach leads to more 
robust and interpretable features, often capturing meaningful patterns in the data.">
  <link rel="icon" href="/favicon.png" type="image/png">
  <link rel="icon" href="/favicon.svg" type="image/svg+xml">
  <link rel="canonical" href="https://mbottoni.github.io/2024/11/10/sae.html">
  <link rel="alternate" type="application/rss+xml" title="mbottoni" href="https://mbottoni.github.io/feed.xml">
  <style>
  @font-face {
    font-family: 'Open Sans'; src: url('/css/OpenSans-300-Normal.woff2') format('woff2');
    font-weight: 300; font-style: normal;
  }
  @font-face {
    font-family: 'JetBrains Mono'; src: url('/css/JetBrainsMono-400-Normal.woff2') format('woff2');
    font-weight: 400; font-style: normal;
  }
  @font-face {
    font-family: 'JetBrains Mono'; src: url('/css/JetBrainsMono-700-Normal.woff2') format('woff2');
    font-weight: 700; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-400-Normal.woff2') format('woff2');
    font-weight: 400; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-400-Italic.woff2') format('woff2');
    font-weight: 400; font-style: italic;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-700-Normal.woff2') format('woff2');
    font-weight: 700; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-700-Italic.woff2') format('woff2');
    font-weight: 700; font-style: italic;
  }

  * { box-sizing: border-box; margin: 0; padding: 0; margin-block-start: 0; margin-block-end: 0; }

  body {
    max-width: 80ch;
    padding: 2ch;
    margin-left: auto;
    margin-right: auto;
  }

  header { margin-bottom: 2rem; }
  header > nav { display: flex; column-gap: 2ch; align-items: baseline; flex-wrap: wrap; }
  header a { font-style: normal; color: rgba(0, 0, 0, .8); text-decoration: none; }
  header a:hover { color: rgba(0, 0, 0, .8); text-decoration: underline; }
  header .title { font-size: 1.25em; flex-grow: 2; }

  footer { margin-top: 2rem; }
  footer > p { display: flex; column-gap: 2ch; justify-content: center; flex-wrap: wrap; }
  footer a { color: rgba(0, 0, 0, .8); text-decoration: none; white-space: nowrap; }
  footer i { vertical-align: middle; color: rgba(0, 0, 0, .8) }

  </style>

  <link rel="stylesheet" href="/css/main.css">
  
</head>

<body>
  <header>
    <nav>
      <a class="title" href="/">mbottoni</a>
      <a href="/about.html">About</a>
      <a href="/resume.html">Resume</a>
      <a href="/links.html">Links</a>
    </nav>
  </header>

  <main>
  <article >

    <h1>
    <a href="#Sparse-Autoencoders"><span>Sparse Autoencoders</span> <time datetime="2024-11-10">Nov 10, 2024</time></a>
    </h1>
<p><span>Sparse autoencoders are neural networks that learn compressed representations of </span>
<span>data while enforcing sparsity - a constraint that ensures most neurons </span>
<span>remain inactive for any given input. This approach leads to more </span>
<span>robust and interpretable features, often capturing meaningful patterns in the data.</span></p>
<section id="Concepts">

    <h2>
    <a href="#Concepts"><span>Concepts</span> </a>
    </h2>
<p><span>The key idea behind sparse autoencoders is adding a sparsity penalty to the </span>
<span>standard autoencoder loss function. For a target sparsity level ρ (typically 0.05 or lower), we want </span>
<span>the average activation of each hidden neuron ρ̂ to be close to ρ. The network achieves this by minimizing two components:</span></p>
<ol>
<li>
<span>Reconstruction Error: How well the network reconstructs the input</span>
</li>
<li>
<span>Sparsity Penalty: How close the average activations are to the target sparsity</span>
</li>
</ol>
<p><span>Here</span>&rsquo;<span>s a simple pseudocode implementation to illustrate these concepts:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">class</span> <span class="hl-title class_">SparseAutoencoder</span>:</span>
<span class="line">    <span class="hl-keyword">def</span> <span class="hl-title function_">__init__</span>(<span class="hl-params">self, input_dim, hidden_dim, rho=<span class="hl-number">0.05</span>, beta=<span class="hl-number">1.0</span></span>):</span>
<span class="line">        <span class="hl-comment"># rho: target sparsity level</span></span>
<span class="line">        <span class="hl-comment"># beta: weight of sparsity penalty</span></span>
<span class="line">        </span>
<span class="line">        <span class="hl-comment"># Initialize weights and biases</span></span>
<span class="line">        self.W1 = random_matrix(input_dim, hidden_dim)</span>
<span class="line">        self.b1 = zeros(hidden_dim)</span>
<span class="line">        self.W2 = random_matrix(hidden_dim, input_dim)</span>
<span class="line">        self.b2 = zeros(input_dim)</span>
<span class="line"></span>
<span class="line">    <span class="hl-keyword">def</span> <span class="hl-title function_">forward</span>(<span class="hl-params">self, X</span>):</span>
<span class="line">        <span class="hl-comment"># Encoder</span></span>
<span class="line">        hidden = sigmoid(dot(X, self.W1) + self.b1)</span>
<span class="line">        </span>
<span class="line">        <span class="hl-comment"># Decoder</span></span>
<span class="line">        output = sigmoid(dot(hidden, self.W2) + self.b2)</span>
<span class="line">        </span>
<span class="line">        <span class="hl-comment"># Calculate average activations for sparsity</span></span>
<span class="line">        rho_hat = mean(hidden, axis=<span class="hl-number">0</span>)</span>
<span class="line">        </span>
<span class="line">        <span class="hl-keyword">return</span> output, hidden, rho_hat</span>
<span class="line"></span>
<span class="line">    <span class="hl-keyword">def</span> <span class="hl-title function_">loss</span>(<span class="hl-params">self, X, output, rho_hat</span>):</span>
<span class="line">        <span class="hl-comment"># Reconstruction error</span></span>
<span class="line">        reconstruction_loss = mean_squared_error(X, output)</span>
<span class="line">        </span>
<span class="line">        <span class="hl-comment"># Sparsity penalty (KL divergence)</span></span>
<span class="line">        sparsity_penalty = <span class="hl-built_in">sum</span>(</span>
<span class="line">            self.rho * log(self.rho / rho_hat) +</span>
<span class="line">            (<span class="hl-number">1</span> - self.rho) * log((<span class="hl-number">1</span> - self.rho) / (<span class="hl-number">1</span> - rho_hat))</span>
<span class="line">        )</span>
<span class="line">        </span>
<span class="line">        <span class="hl-keyword">return</span> reconstruction_loss + self.beta * sparsity_penalty</span></code></pre>

</figure>
<p><span>Let</span>&rsquo;<span>s look at a concrete example. Imagine we</span>&rsquo;<span>re processing MNIST digits:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-comment"># Example usage with MNIST</span></span>
<span class="line">input_dim = <span class="hl-number">784</span>  <span class="hl-comment"># 28x28 pixels</span></span>
<span class="line">hidden_dim = <span class="hl-number">196</span>  <span class="hl-comment"># Compressed representation</span></span>
<span class="line">autoencoder = SparseAutoencoder(input_dim, hidden_dim)</span>
<span class="line"></span>
<span class="line"><span class="hl-comment"># Training loop example</span></span>
<span class="line"><span class="hl-keyword">for</span> epoch <span class="hl-keyword">in</span> <span class="hl-built_in">range</span>(num_epochs):</span>
<span class="line">    <span class="hl-keyword">for</span> batch <span class="hl-keyword">in</span> data_loader:</span>
<span class="line">        <span class="hl-comment"># Forward pass</span></span>
<span class="line">        output, hidden, rho_hat = autoencoder.forward(batch)</span>
<span class="line">        </span>
<span class="line">        <span class="hl-comment"># Calculate loss</span></span>
<span class="line">        loss = autoencoder.loss(batch, output, rho_hat)</span>
<span class="line">        </span>
<span class="line">        <span class="hl-comment"># Update weights using backpropagation</span></span>
<span class="line">        gradients = calculate_gradients(loss)</span>
<span class="line">        update_weights(gradients)</span></code></pre>

</figure>
</section>
<section id="Practical-Examples">

    <h2>
    <a href="#Practical-Examples"><span>Practical Examples</span> </a>
    </h2>
<p><span>Let</span>&rsquo;<span>s look at some typical activation patterns:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-comment"># Dense (standard) autoencoder activation example</span></span>
<span class="line"><span class="hl-comment"># [0.6, 0.4, 0.5, 0.3, 0.7, 0.4, 0.6, 0.5]</span></span>
<span class="line"><span class="hl-comment"># Many neurons are active simultaneously</span></span>
<span class="line"></span>
<span class="line"><span class="hl-comment"># Sparse autoencoder activation example</span></span>
<span class="line"><span class="hl-comment"># [0.01, 0.8, 0.02, 0.01, 0.03, 0.9, 0.02, 0.01]</span></span>
<span class="line"><span class="hl-comment"># Only a few neurons are strongly active</span></span></code></pre>

</figure>
<p><span>Consider an image processing task:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-comment"># Example: Processing a 32x32 image patch</span></span>
<span class="line">patch = load_image_patch(<span class="hl-number">32</span>, <span class="hl-number">32</span>)  <span class="hl-comment"># 1024 dimensions</span></span>
<span class="line"></span>
<span class="line"><span class="hl-comment"># Configure sparse autoencoder</span></span>
<span class="line">encoder = SparseAutoencoder(</span>
<span class="line">    input_dim=<span class="hl-number">1024</span>,</span>
<span class="line">    hidden_dim=<span class="hl-number">256</span>,  <span class="hl-comment"># 4x compression</span></span>
<span class="line">    rho=<span class="hl-number">0.05</span>,  <span class="hl-comment"># Expect 5% average activation</span></span>
<span class="line">    beta=<span class="hl-number">1.0</span></span>
<span class="line">)</span>
<span class="line"></span>
<span class="line"><span class="hl-comment"># Process the patch</span></span>
<span class="line">reconstructed, hidden_repr, activations = encoder.forward(patch)</span>
<span class="line"></span>
<span class="line"><span class="hl-comment"># Example hidden representation might look like:</span></span>
<span class="line"><span class="hl-comment"># Most values near 0, with few strong activations:</span></span>
<span class="line"><span class="hl-comment"># [0.01, 0.00, 0.87, 0.02, 0.00, 0.92, ...]</span></span></code></pre>

</figure>
</section>
<section id="Implementation-Tips">

    <h2>
    <a href="#Implementation-Tips"><span>Implementation Tips</span> </a>
    </h2>
<ol>
<li>
<strong><strong><span>Initialization</span></strong></strong><span>: Initialize weights using a normal distribution with small variance (e.g., 0.01) to avoid saturation:</span>
</li>
</ol>

<figure class="code-block">


<pre><code><span class="line">W = numpy.random.normal(<span class="hl-number">0</span>, <span class="hl-number">0.01</span>, (input_dim, hidden_dim))</span></code></pre>

</figure>
<ol start="2">
<li>
<strong><strong><span>Monitoring</span></strong></strong><span>: Track both reconstruction error and average activations:</span>
</li>
</ol>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">monitor_sparsity</span>(<span class="hl-params">hidden_activations</span>):</span>
<span class="line">    avg_activation = numpy.mean(hidden_activations)</span>
<span class="line">    active_neurons = numpy.mean(hidden_activations &gt; <span class="hl-number">0.1</span>)</span>
<span class="line">    <span class="hl-built_in">print</span>(<span class="hl-string">f&quot;Average activation: <span class="hl-subst">{avg_activation:<span class="hl-number">.3</span>f}</span>&quot;</span>)</span>
<span class="line">    <span class="hl-built_in">print</span>(<span class="hl-string">f&quot;Proportion of active neurons: <span class="hl-subst">{active_neurons:<span class="hl-number">.3</span>f}</span>&quot;</span>)</span></code></pre>

</figure>
<ol start="3">
<li>
<strong><strong><span>Hyperparameter Selection</span></strong></strong><span>:</span>
</li>
</ol>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-comment"># Conservative starting points</span></span>
<span class="line">hyperparams = {</span>
<span class="line">    <span class="hl-string">&#x27;learning_rate&#x27;</span>: <span class="hl-number">0.001</span>,</span>
<span class="line">    <span class="hl-string">&#x27;rho&#x27;</span>: <span class="hl-number">0.05</span>,        <span class="hl-comment"># Target sparsity</span></span>
<span class="line">    <span class="hl-string">&#x27;beta&#x27;</span>: <span class="hl-number">1.0</span>,        <span class="hl-comment"># Sparsity weight</span></span>
<span class="line">    <span class="hl-string">&#x27;batch_size&#x27;</span>: <span class="hl-number">128</span>,</span>
<span class="line">    <span class="hl-string">&#x27;hidden_dim&#x27;</span>: input_dim // <span class="hl-number">4</span>  <span class="hl-comment"># 4x compression</span></span>
<span class="line">}</span></code></pre>

</figure>
<p><span>The power of sparse autoencoders lies in their ability to discover specialized feature </span>
<span>detectors. Each neuron becomes sensitive to specific patterns in the input </span>
<span>data, making the learned representations more interpretable and often more </span>
<span>useful for downstream tasks like classification or anomaly detection.</span></p>
<p><span>Remember that the sparsity constraint isn</span>&rsquo;<span>t about having fewer neurons, but rather </span>
<span>about having fewer neurons active at once. This mimics biological neural networks, where energy efficiency is </span>
<span>achieved through sparse activation patterns.</span></p>
</section>
</article>
  </main>

  <footer class="site-footer">
    <p>
      <a href="https://github.com/mbottoni/mbottoni.github.io/edit/master/content/posts/2024-11-10-sae.dj">
        <svg class="icon"><use href="/assets/icons.svg#edit"/></svg>
        Fix typo
      </a>
      <a href="/feed.xml">
        <svg class="icon"><use href="/assets/icons.svg#rss"/></svg>
        Subscribe
      </a>
      <a href="mailto:maruanbakriottoni@gmail.com">
        <svg class="icon"><use href="/assets/icons.svg#email"/></svg>
        Get in touch
      </a>
      <a href="https://github.com/mbottoni">
        <svg class="icon"><use href="/assets/icons.svg#github"/></svg>
        mbottoni
      </a>
    </p>
  </footer>
</body>

</html>
