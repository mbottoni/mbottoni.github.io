
<!DOCTYPE html>
<html lang='en-US'>
<head>
  <meta charset='utf-8'>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Self Supervised Learning</title>
  <meta name="description" content="This post is based on this blog post by meta Link to post.">
  <link rel="icon" href="/favicon.png" type="image/png">
  <link rel="icon" href="/favicon.svg" type="image/svg+xml">
  <link rel="canonical" href="https://mbottoni.github.io/2024/08/18/ssl.html">
  <link rel="alternate" type="application/rss+xml" title="mbottoni" href="https://mbottoni.github.io/feed.xml">
  <style>
  @font-face {
    font-family: 'Open Sans'; src: url('/css/OpenSans-300-Normal.woff2') format('woff2');
    font-weight: 300; font-style: normal;
  }
  @font-face {
    font-family: 'JetBrains Mono'; src: url('/css/JetBrainsMono-400-Normal.woff2') format('woff2');
    font-weight: 400; font-style: normal;
  }
  @font-face {
    font-family: 'JetBrains Mono'; src: url('/css/JetBrainsMono-700-Normal.woff2') format('woff2');
    font-weight: 700; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-400-Normal.woff2') format('woff2');
    font-weight: 400; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-400-Italic.woff2') format('woff2');
    font-weight: 400; font-style: italic;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-700-Normal.woff2') format('woff2');
    font-weight: 700; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-700-Italic.woff2') format('woff2');
    font-weight: 700; font-style: italic;
  }

  * { box-sizing: border-box; margin: 0; padding: 0; margin-block-start: 0; margin-block-end: 0; }

  body {
    max-width: 80ch;
    padding: 2ch;
    margin-left: auto;
    margin-right: auto;
  }

  header { margin-bottom: 2rem; }
  header > nav { display: flex; column-gap: 2ch; align-items: baseline; flex-wrap: wrap; }
  header a { font-style: normal; color: rgba(0, 0, 0, .8); text-decoration: none; }
  header a:hover { color: rgba(0, 0, 0, .8); text-decoration: underline; }
  header .title { font-size: 1.25em; flex-grow: 2; }

  footer { margin-top: 2rem; }
  footer > p { display: flex; column-gap: 2ch; justify-content: center; flex-wrap: wrap; }
  footer a { color: rgba(0, 0, 0, .8); text-decoration: none; white-space: nowrap; }
  footer i { vertical-align: middle; color: rgba(0, 0, 0, .8) }

  </style>

  <link rel="stylesheet" href="/css/main.css">
  
</head>

<body>
  <header>
    <nav>
      <a class="title" href="/">mbottoni</a>
      <a href="/about.html">About</a>
      <a href="/resume.html">Resume</a>
      <a href="/links.html">Links</a>
    </nav>
  </header>

  <main>
  <article >

    <h1>
    <a href="#Self-Supervised-Learning"><span>Self Supervised Learning</span> <time datetime="2024-08-18">Aug 18, 2024</time></a>
    </h1>

<figure>

<img alt="" src="/assets/ssl.webp">
</figure>
<p><span>This post is based on this blog post by meta </span><a href="https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/"><span>Link to post</span></a><span>.</span></p>
<p><span>In recent years, AI has seen remarkable advancements, particularly in the development of AI systems </span>
<span>that thrive on massive amounts of labeled data. This approach, known as supervised learning, has </span>
<span>proven highly effective for training specialized models to perform specific tasks with incredible </span>
<span>precision. However, while supervised learning has brought us this far, it has its limitations.</span></p>
<p><span>Particularly for creating generalist models—those capable of performing multiple tasks and acquiring </span>
<span>new skills without needing large amounts of labeled data. The challenge is simple: it’s impossible </span>
<span>to label everything in the world. Additionally, for certain tasks, there simply isn</span>&rsquo;<span>t enough </span>
<span>labeled data available. For example, training AI to understand low-resource languages faces </span>
<span>significant challenges due to the scarcity of labeled examples.</span></p>
<p><span>To truly advance AI toward human-level intelligence, we need systems that can learn and adapt beyond the </span>
<span>confines of their training data. This would involve AI systems developing a more nuanced understanding </span>
<span>of the world, similar to how humans learn.</span></p>
<section id="The-Role-of-Common-Sense-in-Intelligence">

    <h2>
    <a href="#The-Role-of-Common-Sense-in-Intelligence"><span>The Role of Common Sense in Intelligence</span> </a>
    </h2>
<p><span>As humans, we learn about the world largely through observation and interaction. From an early age, we develop generalized </span>
<span>predictive models about our environment, learning fundamental concepts like object permanence and gravity. This generalized knowledge, often referred </span>
<span>to as common sense, is a cornerstone of human intelligence, enabling us to learn new skills with minimal instruction.</span></p>
<p><span>For AI to reach similar levels of understanding and adaptability, it needs to acquire a form of common sense—something that </span>
<span>has remained a significant challenge in AI research. The question then becomes: how can we equip machines with this kind of background knowledge?</span>
<span>The answers lies on SSL (Self Supervised Learning)</span></p>
</section>
<section id="Enter-Self-Supervised-Learning">

    <h2>
    <a href="#Enter-Self-Supervised-Learning"><span>Enter Self-Supervised Learning</span> </a>
    </h2>
<p><span>One of the most promising approaches to addressing this challenge is self-supervised learning (SSL). Unlike supervised learning, which relies on </span>
<span>labeled data, self-supervised learning allows AI systems to learn from vast amounts of unlabeled data. This approach is crucial for recognizing </span>
<span>and understanding more subtle, less common patterns in the world.</span></p>
<p><span>Self-supervised learning has already made significant strides in natural language processing (NLP). Models like Word2Vec, GloVe, BERT, RoBERTa, and others have been </span>
<span>trained on large, unlabeled text datasets, resulting in systems that outperform those trained solely with supervised learning.</span></p>
<p><span>While SSL has seen considerable success in NLP, its application in computer vision (CV) has been more challenging. Representing uncertainty in image </span>
<span>predictions is significantly more complex than in text. However, recent developments are starting to overcome these hurdles.</span>
<span>A notable example is the SEER project, which leverages SSL to pretrain a large network on a billion random, unlabeled images. This approach has </span>
<span>led to top accuracy on a diverse set of vision tasks, demonstrating that SSL can excel in complex, real-world CV settings as well.</span></p>
</section>
<section id="Energy-Based-Models">

    <h2>
    <a href="#Energy-Based-Models"><span>Energy Based Models</span> </a>
    </h2>
<p><span>One of the more exciting developments in SSL is the exploration of energy-based models (EBMs). EBMs measure the compatibility between </span>
<span>inputs, making them particularly well-suited for tasks where prediction uncertainty is high, such as in CV. By training these models </span>
<span>to recognize compatible and incompatible pairs of inputs, we can create systems that are better at making nuanced predictions.</span></p>
<p><span>Additionally, the use of joint embedding architectures, like Siamese networks, has shown promise in preventing the </span>
<span>collapse of model training, where the system fails to differentiate between similar but distinct </span>
<span>inputs. These architectures are being refined with both contrastive and non-contrastive methods </span>
<span>to improve their effectiveness in SSL.</span></p>
</section>
<section id="The-future">

    <h2>
    <a href="#The-future"><span>The future</span> </a>
    </h2>
<p><span>Looking ahead, the challenge for AI researchers is to develop non-contrastive methods for latent-variable energy-based models that can </span>
<span>produce robust representations of images, videos, and other signals. Achieving this will be a significant step toward creating AI systems that can </span>
<span>learn and adapt with minimal labeled data, moving us closer to human-level intelligence.</span></p>
</section>
</article>
  </main>

  <footer class="site-footer">
    <p>
      <a href="https://github.com/mbottoni/mbottoni.github.io/edit/master/content/posts/2024-08-18-ssl.dj">
        <svg class="icon"><use href="/assets/icons.svg#edit"/></svg>
        Fix typo
      </a>
      <a href="/feed.xml">
        <svg class="icon"><use href="/assets/icons.svg#rss"/></svg>
        Subscribe
      </a>
      <a href="mailto:maruanbakriottoni@gmail.com">
        <svg class="icon"><use href="/assets/icons.svg#email"/></svg>
        Get in touch
      </a>
      <a href="https://github.com/mbottoni">
        <svg class="icon"><use href="/assets/icons.svg#github"/></svg>
        mbottoni
      </a>
    </p>
  </footer>
</body>

</html>
