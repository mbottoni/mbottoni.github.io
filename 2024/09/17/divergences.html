
<!DOCTYPE html>
<html lang='en-US'>
<head>
  <meta charset='utf-8'>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Some common divergences</title>
  <meta name="description" content="Hey. usually on machine learning we have to consider distances between different probability distributions.
This is in fact a hard problem and there is a misconception that the most common way to do this is using the
kl divergence. I believe that this is not true.">
  <link rel="icon" href="/favicon.png" type="image/png">
  <link rel="icon" href="/favicon.svg" type="image/svg+xml">
  <link rel="canonical" href="https://mbottoni.github.io/2024/09/17/divergences.html">
  <link rel="alternate" type="application/rss+xml" title="mbottoni" href="https://mbottoni.github.io/feed.xml">
  <style>
  @font-face {
    font-family: 'Open Sans'; src: url('/css/OpenSans-300-Normal.woff2') format('woff2');
    font-weight: 300; font-style: normal;
  }
  @font-face {
    font-family: 'JetBrains Mono'; src: url('/css/JetBrainsMono-400-Normal.woff2') format('woff2');
    font-weight: 400; font-style: normal;
  }
  @font-face {
    font-family: 'JetBrains Mono'; src: url('/css/JetBrainsMono-700-Normal.woff2') format('woff2');
    font-weight: 700; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-400-Normal.woff2') format('woff2');
    font-weight: 400; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-400-Italic.woff2') format('woff2');
    font-weight: 400; font-style: italic;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-700-Normal.woff2') format('woff2');
    font-weight: 700; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-700-Italic.woff2') format('woff2');
    font-weight: 700; font-style: italic;
  }

  * { box-sizing: border-box; margin: 0; padding: 0; margin-block-start: 0; margin-block-end: 0; }

  body {
    max-width: 80ch;
    padding: 2ch;
    margin-left: auto;
    margin-right: auto;
  }

  header { margin-bottom: 2rem; }
  header > nav { display: flex; column-gap: 2ch; align-items: baseline; flex-wrap: wrap; }
  header a { font-style: normal; color: rgba(0, 0, 0, .8); text-decoration: none; }
  header a:hover { color: rgba(0, 0, 0, .8); text-decoration: underline; }
  header .title { font-size: 1.25em; flex-grow: 2; }

  footer { margin-top: 2rem; }
  footer > p { display: flex; column-gap: 2ch; justify-content: center; flex-wrap: wrap; }
  footer a { color: rgba(0, 0, 0, .8); text-decoration: none; white-space: nowrap; }
  footer i { vertical-align: middle; color: rgba(0, 0, 0, .8) }

  </style>

  <link rel="stylesheet" href="/css/main.css">
  
</head>

<body>
  <header>
    <nav>
      <a class="title" href="/">mbottoni</a>
      <a href="/about.html">About</a>
      <a href="/resume.html">Resume</a>
      <a href="/links.html">Links</a>
    </nav>
  </header>

  <main>
  <article >

    <h1>
    <a href="#Some-common-divergences"><span>Some common divergences</span> <time datetime="2024-09-17">Sep 17, 2024</time></a>
    </h1>
<p><span>Hey. usually on machine learning we have to consider distances between different probability distributions.</span>
<span>This is in fact a hard problem and there is a misconception that the most common way to do this is using the</span>
<span>kl divergence. I believe that this is not true.</span></p>
<p><span>In fact, the KL divergence is not a metric, because it is not symmetric. It have nice properties that makes it useful</span>
<span>but there are other metrics that are more useful in specific scenarios. Sadly these metrics are not taught on introductory</span>
<span>courses in machine learning but there is a research trend on using these metrics on common architectures where you usually use </span>
<span>the KL divergence on the loss, suck as the VAE</span>&rsquo;<span>s and DDPMs.</span></p>
<p><span>For example, here is a pseudocode for some common divergences</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">kl_divergence</span>(<span class="hl-params">p, q</span>):</span>
<span class="line">    <span class="hl-string">&quot;&quot;&quot;</span></span>
<span class="line"><span class="hl-string">    Compute the Kullback-Leibler divergence D_KL(P || Q)</span></span>
<span class="line"><span class="hl-string">    &quot;&quot;&quot;</span></span>
<span class="line">    p = np.asarray(p, dtype=np.float64)</span>
<span class="line">    q = np.asarray(q, dtype=np.float64)</span>
<span class="line">    epsilon = <span class="hl-number">1e-10</span></span>
<span class="line">    p = np.clip(p, epsilon, <span class="hl-number">1</span>)</span>
<span class="line">    q = np.clip(q, epsilon, <span class="hl-number">1</span>)</span>
<span class="line">    <span class="hl-keyword">return</span> np.<span class="hl-built_in">sum</span>(p * np.log(p / q))</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">js_divergence</span>(<span class="hl-params">p, q</span>):</span>
<span class="line">    <span class="hl-string">&quot;&quot;&quot;</span></span>
<span class="line"><span class="hl-string">    Compute the Jensen-Shannon divergence between distributions P and Q</span></span>
<span class="line"><span class="hl-string">    &quot;&quot;&quot;</span></span>
<span class="line">    m = <span class="hl-number">0.5</span> * (p + q)</span>
<span class="line">    <span class="hl-keyword">return</span> <span class="hl-number">0.5</span> * kl_divergence(p, m) + <span class="hl-number">0.5</span> * kl_divergence(q, m)</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">hellinger_distance</span>(<span class="hl-params">p, q</span>):</span>
<span class="line">    <span class="hl-string">&quot;&quot;&quot;</span></span>
<span class="line"><span class="hl-string">    Compute the Hellinger distance between distributions P and Q</span></span>
<span class="line"><span class="hl-string">    &quot;&quot;&quot;</span></span>
<span class="line">    <span class="hl-keyword">return</span> np.sqrt(np.<span class="hl-built_in">sum</span>((np.sqrt(p) - np.sqrt(q))**<span class="hl-number">2</span>)) / np.sqrt(<span class="hl-number">2</span>)</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">total_variation_distance</span>(<span class="hl-params">p, q</span>):</span>
<span class="line">    <span class="hl-string">&quot;&quot;&quot;</span></span>
<span class="line"><span class="hl-string">    Compute the Total Variation distance between distributions P and Q</span></span>
<span class="line"><span class="hl-string">    &quot;&quot;&quot;</span></span>
<span class="line">    <span class="hl-keyword">return</span> <span class="hl-number">0.5</span> * np.<span class="hl-built_in">sum</span>(np.<span class="hl-built_in">abs</span>(p - q))</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">bhattacharyya_distance</span>(<span class="hl-params">p, q</span>):</span>
<span class="line">    <span class="hl-string">&quot;&quot;&quot;</span></span>
<span class="line"><span class="hl-string">    Compute the Bhattacharyya distance between distributions P and Q</span></span>
<span class="line"><span class="hl-string">    &quot;&quot;&quot;</span></span>
<span class="line">    bc = np.<span class="hl-built_in">sum</span>(np.sqrt(p * q))</span>
<span class="line">    <span class="hl-keyword">return</span> -np.log(bc)</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">cosine_distance</span>(<span class="hl-params">p, q</span>):</span>
<span class="line">    <span class="hl-string">&quot;&quot;&quot;</span></span>
<span class="line"><span class="hl-string">    Compute the Cosine distance between distributions P and Q</span></span>
<span class="line"><span class="hl-string">    &quot;&quot;&quot;</span></span>
<span class="line">    <span class="hl-keyword">return</span> cosine(p, q)</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">euclidean_distance</span>(<span class="hl-params">p, q</span>):</span>
<span class="line">    <span class="hl-string">&quot;&quot;&quot;</span></span>
<span class="line"><span class="hl-string">    Compute the Euclidean distance between distributions P and Q</span></span>
<span class="line"><span class="hl-string">    &quot;&quot;&quot;</span></span>
<span class="line">    <span class="hl-keyword">return</span> euclidean(p, q)</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">chi_squared_distance</span>(<span class="hl-params">p, q</span>):</span>
<span class="line">    <span class="hl-string">&quot;&quot;&quot;</span></span>
<span class="line"><span class="hl-string">    Compute the Chi-Squared distance between distributions P and Q</span></span>
<span class="line"><span class="hl-string">    &quot;&quot;&quot;</span></span>
<span class="line">    epsilon = <span class="hl-number">1e-10</span></span>
<span class="line">    p = np.asarray(p, dtype=np.float64) + epsilon</span>
<span class="line">    q = np.asarray(q, dtype=np.float64) + epsilon</span>
<span class="line">    <span class="hl-keyword">return</span> <span class="hl-number">0.5</span> * np.<span class="hl-built_in">sum</span>(((p - q)**<span class="hl-number">2</span>) / (p + q))</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">kolmogorov_smirnov_statistic</span>(<span class="hl-params">p, q</span>):</span>
<span class="line">    <span class="hl-string">&quot;&quot;&quot;</span></span>
<span class="line"><span class="hl-string">    Compute the Kolmogorov-Smirnov statistic between distributions P and Q</span></span>
<span class="line"><span class="hl-string">    &quot;&quot;&quot;</span></span>
<span class="line">    cdf_p = np.cumsum(p)</span>
<span class="line">    cdf_q = np.cumsum(q)</span>
<span class="line">    <span class="hl-keyword">return</span> np.<span class="hl-built_in">max</span>(np.<span class="hl-built_in">abs</span>(cdf_p - cdf_q))</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">mahalanobis_distance</span>(<span class="hl-params">p, q, cov=<span class="hl-literal">None</span></span>):</span>
<span class="line">    <span class="hl-string">&quot;&quot;&quot;</span></span>
<span class="line"><span class="hl-string">    Compute the Mahalanobis distance between distributions P and Q</span></span>
<span class="line"><span class="hl-string">    &quot;&quot;&quot;</span></span>
<span class="line">    diff = p - q</span>
<span class="line">    <span class="hl-keyword">if</span> cov <span class="hl-keyword">is</span> <span class="hl-literal">None</span>:</span>
<span class="line">        cov = np.cov(np.stack([p, q], axis=<span class="hl-number">0</span>).T)</span>
<span class="line">    cov_inv = inv(cov)</span>
<span class="line">    <span class="hl-keyword">return</span> np.sqrt(np.dot(np.dot(diff.T, cov_inv), diff))</span></code></pre>

</figure>
<p><span>Anyway, for more details here is the colab where I implemented some of these metrics and more </span><a href="https://colab.research.google.com/drive/11WsBPSsF00E0w6FsRsU-bT_gm-KjCvGr?usp=sharing"><span>Link to code</span></a></p>
</article>
  </main>

  <footer class="site-footer">
    <p>
      <a href="https://github.com/mbottoni/mbottoni.github.io/edit/master/content/posts/2024-09-17-divergences.dj">
        <svg class="icon"><use href="/assets/icons.svg#edit"/></svg>
        Fix typo
      </a>
      <a href="/feed.xml">
        <svg class="icon"><use href="/assets/icons.svg#rss"/></svg>
        Subscribe
      </a>
      <a href="mailto:maruanbakriottoni@gmail.com">
        <svg class="icon"><use href="/assets/icons.svg#email"/></svg>
        Get in touch
      </a>
      <a href="https://github.com/mbottoni">
        <svg class="icon"><use href="/assets/icons.svg#github"/></svg>
        mbottoni
      </a>
    </p>
  </footer>
</body>

</html>
