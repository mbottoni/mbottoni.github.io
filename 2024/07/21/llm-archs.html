
<!DOCTYPE html>
<html lang='en-US'>
<head>
  <meta charset='utf-8'>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Encoder vs Decoder vs EncoderDecoder Architectures</title>
  <meta name="description" content="Language models are a crucial component in natural language processing (NLP). The architecture of these models 
can be broadly categorized into three types: encoder-only, decoder-only, and encoder-decoder architectures. Each of these 
architectures has distinct characteristics and applications.">
  <link rel="icon" href="/favicon.png" type="image/png">
  <link rel="icon" href="/favicon.svg" type="image/svg+xml">
  <link rel="canonical" href="https://mbottoni.github.io/2024/07/21/llm-archs.html">
  <link rel="alternate" type="application/rss+xml" title="mbottoni" href="https://mbottoni.github.io/feed.xml">
  <style>
  @font-face {
    font-family: 'Open Sans'; src: url('/css/OpenSans-300-Normal.woff2') format('woff2');
    font-weight: 300; font-style: normal;
  }
  @font-face {
    font-family: 'JetBrains Mono'; src: url('/css/JetBrainsMono-400-Normal.woff2') format('woff2');
    font-weight: 400; font-style: normal;
  }
  @font-face {
    font-family: 'JetBrains Mono'; src: url('/css/JetBrainsMono-700-Normal.woff2') format('woff2');
    font-weight: 700; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-400-Normal.woff2') format('woff2');
    font-weight: 400; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-400-Italic.woff2') format('woff2');
    font-weight: 400; font-style: italic;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-700-Normal.woff2') format('woff2');
    font-weight: 700; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-700-Italic.woff2') format('woff2');
    font-weight: 700; font-style: italic;
  }

  * { box-sizing: border-box; margin: 0; padding: 0; margin-block-start: 0; margin-block-end: 0; }

  body {
    max-width: 80ch;
    padding: 2ch;
    margin-left: auto;
    margin-right: auto;
  }

  header { margin-bottom: 2rem; }
  header > nav { display: flex; column-gap: 2ch; align-items: baseline; flex-wrap: wrap; }
  header a { font-style: normal; color: rgba(0, 0, 0, .8); text-decoration: none; }
  header a:hover { color: rgba(0, 0, 0, .8); text-decoration: underline; }
  header .title { font-size: 1.25em; flex-grow: 2; }

  footer { margin-top: 2rem; }
  footer > p { display: flex; column-gap: 2ch; justify-content: center; flex-wrap: wrap; }
  footer a { color: rgba(0, 0, 0, .8); text-decoration: none; white-space: nowrap; }
  footer i { vertical-align: middle; color: rgba(0, 0, 0, .8) }

  </style>

  <link rel="stylesheet" href="/css/main.css">
  
</head>

<body>
  <header>
    <nav>
      <a class="title" href="/">mbottoni</a>
      <a href="/about.html">About</a>
      <a href="/resume.html">Resume</a>
      <a href="/links.html">Links</a>
    </nav>
  </header>

  <main>
  <article >

    <h1>
    <a href="#Encoder-vs-Decoder-vs-EncoderDecoder-Architectures"><span>Encoder vs Decoder vs EncoderDecoder Architectures</span> <time datetime="2024-07-21">Jul 21, 2024</time></a>
    </h1>

<figure>

<img alt="" src="/assets/llm_tree.jpg">
</figure>
<section id="Differences-Between-Encoder-Only-Decoder-Only-and-Encoder-Decoder-Architectures-for-Language-Models">

    <h2>
    <a href="#Differences-Between-Encoder-Only-Decoder-Only-and-Encoder-Decoder-Architectures-for-Language-Models"><span>Differences Between Encoder-Only, Decoder-Only, and Encoder-Decoder Architectures for Language Models</span> </a>
    </h2>
<p><span>Language models are a crucial component in natural language processing (NLP). The architecture of these models </span>
<span>can be broadly categorized into three types: encoder-only, decoder-only, and encoder-decoder architectures. Each of these </span>
<span>architectures has distinct characteristics and applications.</span></p>
<section id="1-Encoder-Only-Architectures">

    <h3>
    <a href="#1-Encoder-Only-Architectures"><span>1. Encoder-Only Architectures</span> </a>
    </h3>
<p><span>Encoder-only architectures are designed to transform an input sequence into a fixed-size context representation.</span>
<span>These architectures are primarily used for tasks that require understanding the input context, such as </span>
<span>text classification, named entity recognition, and sentiment analysis. Some examples are BERT and RoBERTa.</span></p>
<ul>
<li>
<strong><strong><span>Self-Attention Mechanism</span></strong></strong><span>: Captures dependencies between tokens in the input sequence.</span>
</li>
<li>
<strong><strong><span>Bidirectional Context</span></strong></strong><span>: Can capture context from both past and future tokens simultaneously.</span>
</li>
<li>
<strong><strong><span>Tasks</span></strong></strong><span>: Best suited for tasks where the entire input sequence is available at once.</span>
</li>
</ul>
<section id="Architecture">

    <h4>
    <a href="#Architecture"><span>Architecture</span> </a>
    </h4>
<ul>
<li>
<strong><strong><span>Input Layer</span></strong></strong><span>: The input sequence is tokenized and embedded into continuous vector representations.</span>
</li>
<li>
<strong><strong><span>Encoder Layer</span></strong></strong><span>: Consists of multiple layers of self-attention and feed-forward neural networks. The self-attention mechanism allows the model to weigh the importance of different tokens in the sequence relative to each other.</span>
</li>
<li>
<strong><strong><span>Output Layer</span></strong></strong><span>: The final hidden states from the encoder layers are used for downstream tasks.</span>
</li>
</ul>
</section>
</section>
<section id="2-Decoder-Only-Architectures">

    <h3>
    <a href="#2-Decoder-Only-Architectures"><span>2. Decoder-Only Architectures</span> </a>
    </h3>
<p><span>Decoder-only architectures generate sequences by predicting the next token given the previous tokens. These architectures </span>
<span>are primarily used for tasks that involve text generation, such as language modeling, text </span>
<span>completion, and machine translation (in autoregressive mode). Some examples are the GPT family</span></p>
<ul>
<li>
<strong><strong><span>Masked Self-Attention Mechanism</span></strong></strong><span>: Prevents information flow from future tokens, ensuring autoregressive generation.</span>
</li>
<li>
<strong><strong><span>Unidirectional Context</span></strong></strong><span>: Only considers past tokens for generating the next token.</span>
</li>
<li>
<strong><strong><span>Tasks</span></strong></strong><span>: Best suited for text generation tasks where the output sequence is produced one token at a time.</span>
</li>
</ul>
<section id="Architecture-1">

    <h4>
    <a href="#Architecture-1"><span>Architecture</span> </a>
    </h4>
<ul>
<li>
<strong><strong><span>Input Layer</span></strong></strong><span>: The input is tokenized and embedded into continuous vector representations.</span>
</li>
<li>
<strong><strong><span>Decoder Layer</span></strong></strong><span>: Consists of multiple layers of masked self-attention and feed-forward neural networks. The masked self-attention ensures that the prediction for a given token depends only on the tokens before it.</span>
</li>
<li>
<strong><strong><span>Output Layer</span></strong></strong><span>: Each output token is generated sequentially, based on the context of previously generated tokens.</span>
</li>
</ul>
</section>
</section>
<section id="3-Encoder-Decoder-Architectures">

    <h3>
    <a href="#3-Encoder-Decoder-Architectures"><span>3. Encoder-Decoder Architectures</span> </a>
    </h3>
<p><span>Encoder-decoder architectures, also known as sequence-to-sequence (Seq2Seq) models, are designed to </span>
<span>transform an input sequence into an output sequence. These architectures are used for tasks that involve </span>
<span>transforming one sequence into another, such as machine translation, text summarization, and question answering.</span>
<span>Some examples are the original transformer architecture and T5</span></p>
<ul>
<li>
<strong><strong><span>Cross-Attention Mechanism</span></strong></strong><span>: Allows the decoder to attend to different parts of the input sequence through the encoder</span>&rsquo;<span>s context.</span>
</li>
<li>
<strong><strong><span>Flexible Context Utilization</span></strong></strong><span>: Can handle varying lengths of input and output sequences.</span>
</li>
<li>
<strong><strong><span>Tasks</span></strong></strong><span>: Best suited for tasks where the output sequence is derived from the input sequence.</span>
</li>
</ul>
<section id="Architecture-2">

    <h4>
    <a href="#Architecture-2"><span>Architecture</span> </a>
    </h4>
<ul>
<li>
<strong><strong><span>Encoder</span></strong></strong><span>: Similar to the encoder in encoder-only architectures. It processes the input sequence and generates a context representation.</span>
</li>
<li>
<strong><strong><span>Decoder</span></strong></strong><span>: Similar to the decoder in decoder-only architectures. It generates the output sequence token by token, conditioned on the context representation from the encoder.</span>
</li>
<li>
<strong><strong><span>Attention Mechanism</span></strong></strong><span>: Often includes an additional attention mechanism between the encoder and decoder to allow the decoder to focus on relevant parts of the input sequence.</span>
</li>
</ul>
</section>
</section>
<section id="Comparison-Summary">

    <h3>
    <a href="#Comparison-Summary"><span>Comparison Summary</span> </a>
    </h3>
<table>
<tr>
<th><span>Feature</span></th>
<th><span>Encoder-Only</span></th>
<th><span>Decoder-Only</span></th>
<th><span>Encoder-Decoder</span></th>
</tr>
<tr>
<td><span>Context Direction</span></td>
<td><span>Bidirectional</span></td>
<td><span>Unidirectional (past to future)</span></td>
<td><span>Bidirectional (Encoder) + Unidirectional (Decoder)</span></td>
</tr>
<tr>
<td><span>Self-Attention</span></td>
<td><span>Yes</span></td>
<td><span>Masked</span></td>
<td><span>Yes (Encoder) + Masked (Decoder) + Cross-Attention</span></td>
</tr>
<tr>
<td><span>Primary Use Cases</span></td>
<td><span>Classification, NER, QA</span></td>
<td><span>Text Generation, Language Modeling</span></td>
<td><span>Machine Translation, Summarization, Seq2Seq Tasks</span></td>
</tr>
<tr>
<td><span>Representative Models</span></td>
<td><span>BERT, RoBERTa</span></td>
<td><span>GPT-2, GPT-3</span></td>
<td><span>Transformer, T5, BART</span></td>
</tr>
</table>
</section>
</section>
</article>
  </main>

  <footer class="site-footer">
    <p>
      <a href="https://github.com/mbottoni/mbottoni.github.io/edit/master/content/posts/2024-07-21-llm-archs.dj">
        <svg class="icon"><use href="/assets/icons.svg#edit"/></svg>
        Fix typo
      </a>
      <a href="/feed.xml">
        <svg class="icon"><use href="/assets/icons.svg#rss"/></svg>
        Subscribe
      </a>
      <a href="mailto:maruanbakriottoni@gmail.com">
        <svg class="icon"><use href="/assets/icons.svg#email"/></svg>
        Get in touch
      </a>
      <a href="https://github.com/mbottoni">
        <svg class="icon"><use href="/assets/icons.svg#github"/></svg>
        mbottoni
      </a>
    </p>
  </footer>
</body>

</html>
