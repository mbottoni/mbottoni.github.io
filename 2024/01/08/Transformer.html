
<!DOCTYPE html>
<html lang='en-US'>
<head>
  <meta charset='utf-8'>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Implementing the Transformer in Python</title>
  <meta name="description" content="Hello everyone. 
Today the I will present a sketch of a transformer implementations. 
The focus here will be only on the forward pass of the architecture and not
on learning the weights.">
  <link rel="icon" href="/favicon.png" type="image/png">
  <link rel="icon" href="/favicon.svg" type="image/svg+xml">
  <link rel="canonical" href="https://mbottoni.github.io/2024/01/08/Transformer.html">
  <link rel="alternate" type="application/rss+xml" title="mbottoni" href="https://mbottoni.github.io/feed.xml">
  <style>
  @font-face {
    font-family: 'Open Sans'; src: url('/css/OpenSans-300-Normal.woff2') format('woff2');
    font-weight: 300; font-style: normal;
  }
  @font-face {
    font-family: 'JetBrains Mono'; src: url('/css/JetBrainsMono-400-Normal.woff2') format('woff2');
    font-weight: 400; font-style: normal;
  }
  @font-face {
    font-family: 'JetBrains Mono'; src: url('/css/JetBrainsMono-700-Normal.woff2') format('woff2');
    font-weight: 700; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-400-Normal.woff2') format('woff2');
    font-weight: 400; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-400-Italic.woff2') format('woff2');
    font-weight: 400; font-style: italic;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-700-Normal.woff2') format('woff2');
    font-weight: 700; font-style: normal;
  }
  @font-face {
    font-family: 'EB Garamond'; src: url('/css/EBGaramond-700-Italic.woff2') format('woff2');
    font-weight: 700; font-style: italic;
  }

  * { box-sizing: border-box; margin: 0; padding: 0; margin-block-start: 0; margin-block-end: 0; }

  body {
    max-width: 80ch;
    padding: 2ch;
    margin-left: auto;
    margin-right: auto;
  }

  header { margin-bottom: 2rem; }
  header > nav { display: flex; column-gap: 2ch; align-items: baseline; flex-wrap: wrap; }
  header a { font-style: normal; color: rgba(0, 0, 0, .8); text-decoration: none; }
  header a:hover { color: rgba(0, 0, 0, .8); text-decoration: underline; }
  header .title { font-size: 1.25em; flex-grow: 2; }

  footer { margin-top: 2rem; }
  footer > p { display: flex; column-gap: 2ch; justify-content: center; flex-wrap: wrap; }
  footer a { color: rgba(0, 0, 0, .8); text-decoration: none; white-space: nowrap; }
  footer i { vertical-align: middle; color: rgba(0, 0, 0, .8) }

  </style>

  <link rel="stylesheet" href="/css/main.css">
  
</head>

<body>
  <header>
    <nav>
      <a class="title" href="/">mbottoni</a>
      <a href="/about.html">About</a>
      <a href="/resume.html">Resume</a>
      <a href="/links.html">Links</a>
    </nav>
  </header>

  <main>
  <article >

    <h1>
    <a href="#Implementing-the-Transformer-in-Python"><span>Implementing the Transformer in Python</span> <time datetime="2024-01-08">Jan 8, 2024</time></a>
    </h1>

<figure>

<img alt="" src="/assets/transformer_arch.png">
</figure>
<p><span>Hello everyone. </span>
<span>Today the I will present a sketch of a transformer implementations. </span>
<span>The focus here will be only on the forward pass of the architecture and not</span>
<span>on learning the weights.</span></p>
<p><span>To elaborate further on the technical details of implementing a Transformer model, we can </span>
<span>dive into the key components and steps involved in the forward pass of the </span>
<span>architecture. The Transformer model, introduced in </span>
<span>the paper </span>&ldquo;<span>Attention is All You Need</span>&rdquo;<span> by Vaswani et al., is a deep learning model that </span>
<span>has revolutionized the field of natural language processing (NLP) due to its effectiveness in </span>
<span>handling sequential data without relying on recurrent layers.</span></p>
<section id="Key-Components-of-the-Transformer">

    <h3>
    <a href="#Key-Components-of-the-Transformer"><span>Key Components of the Transformer:</span> </a>
    </h3>
<ol>
<li>
<p><strong><strong><span>Input Embedding</span></strong></strong><span>: The input sequence is converted into a high-dimensional embedding that</span>
<span>represents each token in a continuous vector space. Positional encodings are added to these embeddings </span>
<span>to provide the model with information about the order of tokens.</span></p>
</li>
<li>
<p><strong><strong><span>Multi-Head Attention</span></strong></strong><span>: This mechanism allows the model to focus on different parts of the </span>
<span>input sequence simultaneously. It performs scaled dot-product attention multiple times in</span>
<span>parallel, and the outputs are concatenated and linearly transformed.</span></p>
</li>
<li>
<p><strong><strong><span>Position-wise Feed-Forward Networks</span></strong></strong><span>: Each position in the encoder and decoder layers passes</span>
<span>through a feed-forward neural network, which is applied independently to each position.</span></p>
</li>
<li>
<p><strong><strong><span>Normalization and Residual Connections</span></strong></strong><span>: Each sub-layer (attention and feed-forward networks) in</span>
<span>the encoder and</span>
<span>decoder is followed by layer normalization and is wrapped with a residual connection. This helps in</span>
<span>stabilizing the training of deep networks.</span></p>
</li>
<li>
<p><strong><strong><span>Encoder and Decoder</span></strong></strong><span>: The Transformer model consists of an encoder stack and a decoder</span>
<span>stack. The encoder processes the input sequence, and the decoder generates the output</span>
<span>sequence, one token at a time. The decoder also uses masked attention to prevent future tokens from</span>
<span>being used in the prediction of the current token.</span></p>
</li>
</ol>
</section>
<section id="Implementing-the-Forward-Pass">

    <h3>
    <a href="#Implementing-the-Forward-Pass"><span>Implementing the Forward Pass:</span> </a>
    </h3>
<p><span>The forward pass of the Transformer involves processing the input sequence through the encoder</span>
<span>layers, followed by the decoder layers to generate the output sequence. Here</span>&rsquo;<span>s a high-level overview:</span></p>
<ol>
<li>
<p><strong><strong><span>Prepare Input Embeddings</span></strong></strong><span>: Convert the input tokens into embeddings and add positional encodings.</span></p>
</li>
<li>
<p><strong><strong><span>Encoder</span></strong></strong><span>: For each encoder layer, perform multi-head attention on the input embeddings and</span>
<span>apply the position-wise feed-forward network. Use normalization and residual connections around</span>
<span>each of these sub-layers.</span></p>
</li>
<li>
<p><strong><strong><span>Decoder</span></strong></strong><span>: For each decoder layer, perform masked multi-head attention on the decoder input</span>
<span>embeddings. Then, perform multi-head attention using the encoder output as key and value, and</span>
<span>the output of the previous attention layer as the query. Apply the position-wise feed-forward</span>
<span>network, followed by normalization and residual connections.</span></p>
</li>
<li>
<p><strong><strong><span>Output Linear Layer and Softmax</span></strong></strong><span>: The final decoder output is passed through a linear</span>
<span>layer followed by a softmax function to predict the probability distribution of each token in the</span>
<span>output vocabulary.</span></p>
</li>
</ol>

<figure>

<img alt="" src="/assets/transformer_explanation.png">
</figure>
<p><span>Finally here is the notebook with the implementation: </span><a href="https://colab.research.google.com/drive/1e9jN-S-5LfYs9mVIllh5N_bh8jsvef9P?usp=sharing"><span>Colab</span></a></p>
</section>
</article>
  </main>

  <footer class="site-footer">
    <p>
      <a href="https://github.com/mbottoni/mbottoni.github.io/edit/master/content/posts/2024-01-08-Transformer.dj">
        <svg class="icon"><use href="/assets/icons.svg#edit"/></svg>
        Fix typo
      </a>
      <a href="/feed.xml">
        <svg class="icon"><use href="/assets/icons.svg#rss"/></svg>
        Subscribe
      </a>
      <a href="mailto:maruanbakriottoni@gmail.com">
        <svg class="icon"><use href="/assets/icons.svg#email"/></svg>
        Get in touch
      </a>
      <a href="https://github.com/mbottoni">
        <svg class="icon"><use href="/assets/icons.svg#github"/></svg>
        mbottoni
      </a>
    </p>
  </footer>
</body>

</html>
